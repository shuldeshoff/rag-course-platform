# Модуль 1: Основы LLM

## Цели модуля
- Понять что такое большие языковые модели
- Изучить архитектуру трансформеров
- Освоить основы промпт-инжиниринга
- Узнать о токенизации и эмбеддингах

## Лекция: История и основы LLM

### 1. Введение в LLM
Large Language Model (LLM) - это нейронные сети, обученные на огромных объемах текстовых данных для понимания и генерации естественного языка.

**Ключевые характеристики:**
- Миллиарды параметров
- Обучение на терабайтах текста
- Способность к few-shot learning
- Понимание контекста

### 2. Эволюция LLM
- **2017**: Transformer (Attention is All You Need)
- **2018**: BERT, GPT-1
- **2019**: GPT-2
- **2020**: GPT-3
- **2022**: ChatGPT
- **2023**: GPT-4, LLaMA, YandexGPT

### 3. Архитектура Transformer
```
Input → Tokenization → Embeddings → 
    → Multi-Head Attention → 
    → Feed Forward → 
    → Output
```

**Механизм внимания (Attention):**
- Self-attention позволяет модели учитывать контекст всех слов
- Multi-head attention обрабатывает разные аспекты связей
- Positional encoding добавляет информацию о позиции

### 4. Токенизация
Процесс разбиения текста на токены (подслова):

```
"Привет мир" → ["При", "вет", " мир"] → [1234, 5678, 9012]
```

**Важно:**
- 1 токен ≈ 0.75 слова (для английского)
- 1 токен ≈ 0.5 слова (для русского)
- Стоимость API зависит от токенов

### 5. Эмбеддинги (Embeddings)
Векторное представление текста:

```python
"кот" → [0.2, -0.5, 0.8, ..., 0.1]  # 1024-мерный вектор
"кошка" → [0.3, -0.4, 0.7, ..., 0.2]  # похожий вектор
```

**Свойства:**
- Семантическая близость
- Можно вычислять расстояния
- Основа для поиска

### 6. Промпт-инжиниринг
Искусство создания эффективных запросов к LLM:

**Плохой промпт:**
```
Напиши про RAG
```

**Хороший промпт:**
```
Ты - эксперт по машинному обучению. 
Объясни концепцию RAG простым языком для начинающих.
Включи:
1. Определение
2. Основные компоненты
3. Пример использования
```

### 7. Ограничения LLM
- Галлюцинации (выдумывание фактов)
- Устаревшие данные (cutoff date)
- Высокая стоимость
- Ограничение контекста

**Решение → RAG!**

## Практическое задание

Создайте промпт для YandexGPT, который:
1. Объясняет сложную техническую концепцию
2. Использует аналогии
3. Приводит примеры кода
4. Структурирован и читаем

Сравните результаты с разными температурами (0.3, 0.6, 0.9).

## Тест

1. Что такое токен в контексте LLM?
2. Назовите основные компоненты архитектуры Transformer
3. В чем разница между эмбеддингами слов и токенов?
4. Почему LLM могут "галлюцинировать"?
5. Что означает параметр temperature при генерации?

## Дополнительные материалы
- [Attention is All You Need - оригинальная статья]
- [Illustrated Transformer]
- [YandexGPT Documentation]

